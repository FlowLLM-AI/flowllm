<p align="center">
  <img src="../figure/logo.png" alt="FlowLLM Logo" width="50%">
</p>

<p align="center">
  <strong>FlowLLMï¼šè®©åŸºäºLLMçš„HTTP/MCPæœåŠ¡å¼€å‘æ›´ç®€å•</strong><br>
  <em><sub>å¦‚æœè§‰å¾—æœ‰ç”¨ï¼Œæ¬¢è¿ç»™ä¸ª â­ Starï¼Œæ‚¨çš„æ”¯æŒæ˜¯æˆ‘ä»¬æŒç»­æ”¹è¿›çš„åŠ¨åŠ›</sub></em>
</p>

<p align="center">
  <a href="https://pypi.org/project/flowllm/"><img src="https://img.shields.io/badge/python-3.10+-blue" alt="Python Version"></a>
  <a href="https://pypi.org/project/flowllm/"><img src="https://img.shields.io/badge/pypi-0.2.0.0-blue?logo=pypi" alt="PyPI Version"></a>
  <a href="https://github.com/flowllm-ai/flowllm/blob/main/LICENSE"><img src="https://img.shields.io/badge/license-Apache--2.0-black" alt="License"></a>
  <a href="https://github.com/flowllm-ai/flowllm"><img src="https://img.shields.io/github/stars/flowllm-ai/flowllm?style=social" alt="GitHub Stars"></a>
</p>

---

## ğŸ“– ç®€ä»‹

FlowLLM å°† LLM/Embedding/vector_store èƒ½åŠ›å°è£…ä¸º HTTP/MCP æœåŠ¡ï¼Œé€‚ç”¨äº AI å¯¹è¯åŠ©æ‰‹ã€RAG åº”ç”¨ã€å·¥ä½œæµæœåŠ¡ç­‰åœºæ™¯ï¼Œå¹¶å¯é›†æˆåˆ°æ”¯æŒ
MCP çš„å®¢æˆ·ç«¯å·¥å…·ä¸­ã€‚

### ğŸ—ï¸ æ¶æ„æ¦‚è§ˆ

<p align="center">
  <img src="../figure/framework.png" alt="FlowLLM Framework" width="100%">
</p>


### â­ æ ¸å¿ƒç‰¹æ€§
- **ç®€å•æ˜“ç”¨çš„ Op å¼€å‘**ï¼šç»§æ‰¿ BaseOp æˆ– BaseAsyncOp åŸºç±»ï¼Œå®ç°ä¸šåŠ¡é€»è¾‘å³å¯ã€‚FlowLLMæä¾›äº†å»¶è¿Ÿåˆå§‹åŒ–çš„ LLMã€Embedding æ¨¡å‹å’Œå‘é‡åº“ï¼Œå¼€å‘è€…åªéœ€é€šè¿‡ `self.llm`ã€`self.embedding_model`ã€`self.vector_store` å³å¯è½»æ¾ä½¿ç”¨è¿™äº›èµ„æºã€‚åŒæ—¶FlowLLMæä¾›äº†å®Œæ•´çš„ Prompt æ¨¡æ¿ç®¡ç†èƒ½åŠ›ï¼Œé€šè¿‡ `prompt_format()` å’Œ `get_prompt()` æ–¹æ³•è¿›è¡Œæ ¼å¼åŒ–å’Œä½¿ç”¨ã€‚

- **çµæ´»çš„ Flow ç¼–æ’**ï¼šé€šè¿‡ YAML é…ç½®æ–‡ä»¶å°† Op ç»„åˆæˆ Flowï¼Œæ”¯æŒçµæ´»çš„ç¼–æ’æ–¹å¼ã€‚`>>` è¡¨ç¤ºä¸²è¡Œç»„åˆï¼Œ`|` è¡¨ç¤ºå¹¶è¡Œç»„åˆï¼Œä¾‹å¦‚ `SearchOp() >> (AnalyzeOp() | TranslateOp()) >> FormatOp()` å¯æ„å»ºå¤æ‚çš„å·¥ä½œæµã€‚å®šä¹‰è¾“å…¥è¾“å‡º Schema åï¼Œä½¿ç”¨ `flowllm config=your_config` å‘½ä»¤å³å¯å¯åŠ¨æœåŠ¡ã€‚

- **è‡ªåŠ¨ç”ŸæˆæœåŠ¡**ï¼šé…ç½®å®Œæˆåï¼ŒFlowLLM ä¼šè‡ªåŠ¨ç”Ÿæˆ HTTPã€MCP å’Œ CMD æœåŠ¡ã€‚HTTP æœåŠ¡æä¾›æ ‡å‡†çš„ RESTful APIï¼Œæ”¯æŒåŒæ­¥ JSON å“åº”å’Œ HTTP Stream æµå¼å“åº”ã€‚MCP æœåŠ¡ä¼šè‡ªåŠ¨æ³¨å†Œä¸º Model Context Protocol å·¥å…·ï¼Œå¯é›†æˆåˆ°æ”¯æŒ MCP çš„å®¢æˆ·ç«¯ä¸­ã€‚CMD æœåŠ¡æ”¯æŒå‘½ä»¤è¡Œæ¨¡å¼æ‰§è¡Œå•ä¸ª Opï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•å’Œè°ƒè¯•ã€‚


### ğŸŒŸ åŸºäºFlowLLMçš„åº”ç”¨

| é¡¹ç›®å | æè¿° |
|--------|------|
| [ReMe](https://github.com/agentscope-ai/ReMe) | é¢å‘æ™ºèƒ½ä½“çš„è®°å¿†ç®¡ç†å·¥å…·åŒ… |

---

## âš¡ å¿«é€Ÿå¼€å§‹

### ğŸ“¦ Step0 å®‰è£…

#### ğŸ“¥ From PyPI

```bash
pip install flowllm
```

#### ğŸ”§ From Source

```bash
git clone https://github.com/flowllm-ai/flowllm.git
cd flowllm
pip install -e .
```

è¯¦ç»†å®‰è£…ä¸é…ç½®æ–¹æ³•è¯·å‚è€ƒ [å®‰è£…æŒ‡å—](guide/installation.md)ã€‚

### âš™ï¸ é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼Œé…ç½® API Keyã€‚ä½ å¯ä»¥ä» `example.env` å¤åˆ¶å¹¶ä¿®æ”¹ï¼š

```bash
cp example.env .env
```

ç„¶ååœ¨ `.env` æ–‡ä»¶ä¸­é…ç½®ä½ çš„ API Keyï¼š

```bash
FLOW_LLM_API_KEY=sk-xxxx
FLOW_LLM_BASE_URL=https://xxxx/v1
FLOW_EMBEDDING_API_KEY=sk-xxxx
FLOW_EMBEDDING_BASE_URL=https://xxxx/v1
```

è¯¦ç»†é…ç½®è¯´æ˜è¯·å‚è€ƒ [é…ç½®æŒ‡å—](guide/config_guide.md)ã€‚

### ğŸ› ï¸ Step1 æ„å»ºOp

```python
from flowllm.core.context import C
from flowllm.core.op.base_async_op import BaseAsyncOp
from flowllm.core.schema import Message
from flowllm.core.enumeration import Role

@C.register_op()
class SimpleChatOp(BaseAsyncOp):
    async def async_execute(self):
        query = self.context.get("query", "")
        messages = [Message(role=Role.USER, content=query)]
        response = await self.llm.achat(messages=messages)
        self.context.response.answer = response.content.strip()
```

è¯¦ç»†å†…å®¹è¯·å‚è€ƒ [ç®€å• Op æŒ‡å—](guide/async_op_minimal_guide.md)ã€[LLM Op æŒ‡å—](guide/async_op_llm_guide.md) å’Œ [é«˜çº§ Op æŒ‡å—](guide/async_op_advance_guide.md)ï¼ˆåŒ…å« Embeddingã€VectorStore å’Œå¹¶å‘æ‰§è¡Œç­‰é«˜çº§åŠŸèƒ½ï¼‰ã€‚

### ğŸ“ Step2 é…ç½®config

ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºå¦‚ä½•æ„å»ºä¸€ä¸ª MCPï¼ˆModel Context Protocolï¼‰æœåŠ¡ã€‚åˆ›å»ºé…ç½®æ–‡ä»¶ `my_mcp_config.yaml`ï¼š

```yaml
backend: mcp

mcp:
  transport: sse
  host: "0.0.0.0"
  port: 8001

flow:
  demo_mcp_flow:
    flow_content: MockSearchOp()
    description: "Search results for a given query."
    input_schema:
      query:
        type: string
        description: "User query"
        required: true

llm:
  default:
    backend: openai_compatible
    model_name: qwen3-30b-a3b-instruct-2507
    params:
      temperature: 0.6
```

### ğŸš€ Step3 å¯åŠ¨ MCP æœåŠ¡

```bash
flowllm \
  config=my_mcp_config \
  backend=mcp \  # å¯é€‰ï¼Œè¦†ç›–configé…ç½®
  mcp.transport=sse \  # å¯é€‰ï¼Œè¦†ç›–configé…ç½®
  mcp.port=8001 \  # å¯é€‰ï¼Œè¦†ç›–configé…ç½®
  llm.default.model_name=qwen3-30b-a3b-thinking-2507  # å¯é€‰ï¼Œè¦†ç›–configé…ç½®
```

æœåŠ¡å¯åŠ¨åå¯ä»¥å‚è€ƒ[Client Guide](guide/client_guide.md)æ¥ä½¿ç”¨æœåŠ¡ï¼Œå¯ä»¥ç›´æ¥è·å–æ¨¡å‹æ‰€éœ€è¦çš„tool_callã€‚
